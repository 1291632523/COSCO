{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9645471,"sourceType":"datasetVersion","datasetId":5890539}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torchvision.datasets import VisionDataset\nimport numpy as np\nimport urllib\nfrom os import path, makedirs\n\n\nclass MovingMNIST(VisionDataset):\n    mirrors = [\n        \"https://github.com/eadali/moving-mnist/releases/download/v0.1/\",\n    ]\n\n    resources = [\n        (\"train-sequences.npy\", \"\"),\n        (\"train-annotations.npy\", \"\"),\n    ]\n\n    def __init__(self, root, download=False):\n        self.root = root\n        self.seq_path = path.join(self.root, 'train-sequences.npy')\n        self.ann_path = path.join(self.root, 'train-annotations.npy')\n\n        if download:\n            self.download()\n\n        self.sequences = torch.from_numpy(np.load(self.seq_path))\n        self.annotations = torch.from_numpy(np.load(self.ann_path))\n        #print(self.sequences.shape)\n    def __len__(self):\n        #print(self.sequences.shape[0])\n        return self.sequences.shape[0]\n\n    def __getitem__(self, idx):\n        return self.sequences[idx], self.annotations[idx]\n\n    def download(self):\n        if not path.exists(self.root):\n            makedirs(self.root)\n\n        for filename, md5 in self.resources:\n            for mirror in self.mirrors:\n                url = f\"{mirror}{filename}\"\n                file_path = path.join(self.root, filename)\n                try:\n                    print(f\"Downloading {url}\")\n                    urllib.request.urlretrieve(url, file_path)\n                except urllib.error.URLError as error:\n                    print(f\"Failed to download (trying next):\\n{error}\")\n                    continue\n                finally:\n                    print()\n                break\n            else:\n                raise RuntimeError(f\"Error downloading {filename}\")","metadata":{"_uuid":"3c696e58-e0a0-46fb-8df9-31c6422a6f69","_cell_guid":"9fb81e93-ee60-4d25-86dd-e31f7939a0b8","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nfrom moving_mnist import MovingMNIST\nfrom torch.utils.data import DataLoader, Dataset, random_split\nimport torch.nn.functional as F\n# 检查是否可以使用GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# 加载 MovingMNIST 数据集\nnum_videos = 10000\nsequence_length = 20\nroot = 'data/processed/MovingMNIST/'\nbatch_size = 32\n\n# MovingMNIST dataset loader\ndataset = MovingMNIST(root=root, download=False)\nmnist = DataLoader(dataset, batch_size=num_videos, shuffle=True)\n\n# 将数字移动到新背景上的函数\ndef move_boxes_to_two_sequences(original_sequence, annotation, new_shape=(64, 64)):\n    num_frames, _, height, width = original_sequence.shape\n    new_sequence = torch.zeros((num_frames, 1, new_shape[0], new_shape[1]))  # 新序列\n    labels = []\n\n    for frame_idx in range(num_frames):\n        boxes = annotation[frame_idx]  # 获取当前帧的所有 bounding boxes\n        \n        if len(boxes) >= 2:\n            # 处理第一个box\n            box1 = boxes[0].int()  # 获取第一个box的坐标\n            class_id1, xmin1, ymin1, xmax1, ymax1 = box1\n            digit_image1 = original_sequence[frame_idx, :, ymin1:ymax1, xmin1:xmax1]  # 提取出第一个数字\n            \n            # 将第一个数字插入到新序列的随机位置\n            new_xmin1 = np.random.randint(0, new_shape[1] - (xmax1 - xmin1))\n            new_ymin1 = np.random.randint(0, new_shape[0] - (ymax1 - ymin1))\n            new_xmax1 = new_xmin1 + (xmax1 - xmin1)\n            new_ymax1 = new_ymin1 + (ymax1 - ymin1)\n            new_sequence[frame_idx, :, new_ymin1:new_ymax1, new_xmin1:new_xmax1] = digit_image1\n            labels.append(class_id1.item())  # 记录标签\n            \n\n\n    return new_sequence, labels\n\n# 定义可视化函数\ndef visualize_sequence(sequence, title=\"Video Sequence\"):\n    frames = sequence.cpu().numpy()\n    num_frames = frames.shape[0]\n    \n    plt.figure(figsize=(10, 2))\n    for i in range(num_frames):\n        plt.subplot(1, num_frames, i + 1)\n        plt.imshow(frames[i, 0], cmap='gray')\n        plt.axis('off')\n    plt.suptitle(title)\n    plt.show()\n\n# 提取数据并处理\nsequences, annotations = next(iter(mnist))\nsequences = sequences.squeeze(0)  # [10000, 20, 1, 64, 64]\nannotations = annotations.squeeze(0)  # [10000, 20, boxes, 5]\n\n# 用于存储新生成的 sequences 和标签\nnew_sequences = []\nlabels = []\n\n# 对所有原始 sequences 进行操作\nfor sequence_idx in range(sequences.shape[0]):\n    original_sequence = sequences[sequence_idx]  # 形状为 [20, 1, 64, 64]\n    annotation = annotations[sequence_idx]  # 形状为 [20, boxes, 5]\n\n    # 将两个 box 移动到新序列中\n    new_sequence, label = move_boxes_to_two_sequences(original_sequence, annotation)\n\n    # 存储结果\n    new_sequences.append(new_sequence)\n    labels.append(label[0])  # 取第一个label作为序列的label\n\n# 划分训练集与验证集\ntrain_size = int(0.8 * len(new_sequences))\nval_size = len(new_sequences) - train_size\ntrain_sequences, val_sequences = new_sequences[:train_size], new_sequences[train_size:]\ntrain_labels, val_labels = labels[:train_size], labels[train_size:]\n\n# 将数据转换为Tensor\ntrain_sequences = torch.stack(train_sequences).to(device)  # [train_size, 20, 1, 64, 64]\nval_sequences = torch.stack(val_sequences).to(device)  # [val_size, 20, 1, 64, 64]\ntrain_labels = torch.tensor(train_labels).to(device)  # [train_size]\nval_labels = torch.tensor(val_labels).to(device)  # [val_size]\n\n# 定义数据集类\nclass CustomDataset(Dataset):\n    def __init__(self, sequences, labels):\n        self.sequences = sequences\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        return self.sequences[idx], self.labels[idx]\n\n# 创建数据加载器\ntrain_dataset = CustomDataset(train_sequences, train_labels)\nval_dataset = CustomDataset(val_sequences, val_labels)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n# 定义LSTM模型\nclass Conv3DModel(nn.Module):\n    def __init__(self):\n        super(Conv3DModel, self).__init__()\n        self.conv1 = nn.Conv3d(1, 16, kernel_size=(3, 3, 3), stride=1, padding=1)\n        self.conv2 = nn.Conv3d(16, 32, kernel_size=(3, 3, 3), stride=1, padding=1)\n        self.fc1 = nn.Linear(32 * 20 * 32 * 32*4, 128)  # 32x32是经过卷积后特征图的尺寸\n        self.fc2 = nn.Linear(128, 10)  # 假设有10个类别\n\n    def forward(self, x):\n        x = self.conv1(x)  # [batch_size, 16, 20, 64, 64]\n        x = F.relu(x)\n        x = self.conv2(x)  # [batch_size, 32, 20, 64, 64]\n        x = F.relu(x)\n        x = x.view(x.size(0), -1)  # 展平\n        x = self.fc1(x)  # [batch_size, 128]\n        x = F.relu(x)\n        x = self.fc2(x)  # [batch_size, 10]\n        return x\n\n# 训练模型\ndef train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n    for epoch in range(num_epochs):\n        model.train()  # 确保在训练时使用 train 模式\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for sequences, labels in train_loader:\n            sequences, labels = sequences.permute(0, 2, 1, 3, 4).to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(sequences)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n        \n        train_accuracy = 100 * correct / total\n        val_accuracy = validate_model(model, val_loader, criterion)\n\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, '\n              f'Train Accuracy: {train_accuracy:.2f}%, Val Accuracy: {val_accuracy:.2f}%')\n\n# 验证模型\ndef validate_model(model, val_loader, criterion):\n    model.eval()  # 确保在评估时使用 eval 模式\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for sequences, labels in val_loader:\n            sequences, labels = sequences.permute(0, 2, 1, 3, 4).to(device), labels.to(device)\n            outputs = model(sequences)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    return 100 * correct / total\n\n# 初始化模型、损失函数和优化器\nmodel = Conv3DModel().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# 训练模型\ntrain_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50)\n\n# 可视化其中一个新序列及其标签\nvisualize_sequence(new_sequences[0], title=\"New Video Sequence with Label \" + str(labels[0]))\n","metadata":{},"execution_count":null,"outputs":[]}]}